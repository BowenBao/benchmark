import os
from pathlib import Path

class_models = [
    ('hf_GPT2', (8, 256), 512, 'AutoConfig.from_pretrained("gpt2")', 'AutoModelForCausalLM.from_config'),
    ('hf_T5', (8, 512), 256, 'AutoConfig.from_pretrained("t5-small", bos_token_id=0)', 'AutoModelForSeq2SeqLM.from_config'),
    ('hf_Bart', (8, 256), 256, 'AutoConfig.from_pretrained("facebook/bart-base")', 'AutoModelForMaskedLM.from_config'),
    ('hf_Reformer', (8, 4096), 3072, 'ReformerConfig(is_decoder=True, bos_token_id=0)', 'AutoModelForCausalLM.from_config'),
    ('hf_BigBird', (2, 1024), 2048, 'BigBirdConfig(attention_type="block_sparse",)', 'AutoModelForCausalLM.from_config'),
    ('hf_Albert', (8, 512), 256, 'AutoConfig.from_pretrained("albert-base-v2")', 'AutoModelForMaskedLM.from_config'),
    ('hf_DistilBert', (8, 512), 256, 'AutoConfig.from_pretrained("distilbert-base-uncased")', 'AutoModelForMaskedLM.from_config')
]
for name, input_shape, eval_context, config, model in class_models:
    folder = Path(name)
    if not os.path.isdir(folder):
        os.makedirs(folder)
    init_program = f"""
# Generated by gen_hf_generative.py
import torch
import torch.optim as optim
import torchvision.models as models
from ...util.model import BenchmarkModel
from torchbenchmark.tasks import NLP
from transformers import *
from datasets import load_dataset

class Model(BenchmarkModel):
    task = NLP.LANGUAGE_MODELING

    def __init__(self, device=None, jit=False):
        super().__init__()
        self.device = device
        self.jit = jit


        config = {config}
        self.model = {model}(config).to(device)
        self.optimizer = optim.Adam(self.model.parameters(), lr=0.001)

        num_tokens = 5

        input_ids = torch.randint(0, config.vocab_size, {input_shape}).to(device)
        decoder_ids = torch.randint(0, config.vocab_size, {input_shape}).to(device)

        eval_context = torch.randint(0, config.vocab_size, (1, {eval_context})).to(device)

        self.train_inputs = {{'input_ids': input_ids, 'labels': decoder_ids}}
        self.eval_inputs = {{'input_ids': eval_context, 'min_length': num_tokens + {eval_context}, 'max_length': num_tokens + {eval_context}}}

    def get_module(self):
        if self.jit:
            raise NotImplementedError()
        return self.model, self.eval_inputs

    def train(self, niter=3):
        if self.jit:
            raise NotImplementedError()
        self.model.train()
        for _ in range(niter):
            outputs = self.model(**self.train_inputs)
            loss = outputs.loss
            loss.backward()
            self.optimizer.step()

    def eval(self, niter=1):
        if self.jit:
            raise NotImplementedError()
        self.model.eval()
        with torch.no_grad():
            for _ in range(niter):
                out = self.model.generate(**self.eval_inputs)


if __name__ == "__main__":
    import time
    m = Model(device="cuda")
    module, example_inputs = m.get_module()

    m.train(niter=1)
    torch.cuda.synchronize()

    begin = time.time()
    m.train(niter=1)
    torch.cuda.synchronize()
    print(time.time()-begin)

    begin = time.time()
    m.eval(niter=1)
    torch.cuda.synchronize()
    print(time.time()-begin)
    """
    with open(folder / '__init__.py', 'w') as f:
        f.write(init_program)
    with open(folder / 'install.py', 'w') as f:
        pass
    with open(folder / 'requirements.txt', 'w') as f:
        f.write('transformers\n')
