
"""
Compute the benchmark score given a frozen score configuration and current benchmark data.
"""
import argparse
import json
import yaml

def compute_score(config, data):
    target = config['target']
    score = 1.0
    weight_sum = 0.0
    for name in config['benchmarks']:
        cfg = config['benchmarks'][name]
        weight, norm = cfg['weight'], cfg['norm']
        weight_sum += weight
        measured_mean = [b['stats']['mean'] for b in data['benchmarks'] if b['name'] == name][0]
        benchmark_score = (norm / measured_mean) ** weight
        # print(f"{name}: {benchmark_score}")
        score *= benchmark_score

    score = score ** (1.0 / len(config['benchmarks']))
    assert abs(weight_sum - 1.0) < 1e-6, f"Bad configuration, weights don't sum to 1, but {weight_sum}"
    return score * target    

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description=__doc__)
    parser.add_argument("--configuration", required=True,
        help="frozen benchmark configuration generated by generate_score_config.py")
    parser.add_argument("--benchmark_data", required=True,
        help="pytest-benchmark json file with current benchmark data")
    args = parser.parse_args()

    with open(args.configuration) as cfg_file:
        config = yaml.full_load(cfg_file)

    with open(args.benchmark_data) as data_file:
        data = json.load(data_file)

    score = compute_score(config, data)
    print(score)
    # print(f"Benchmark Score: {score} (rounded) {int(round(score))}")